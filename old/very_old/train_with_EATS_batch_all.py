#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Aug 14 19:15:30 2019

USAGE:
    CUDA_VISIBLE_DEVICES=0 python train_batch_hard.py <opt1:(str)EXP_NAME> <opt2:(int)EPOCH> <opt3:(str)TEST_MODE>

OPTIONS:
    - If <opt1:EXP_NAME> is given and pretrained model exists:
        - If <opt2> is empty, it continues training from the latest checkpoint.
        - Else if <opt2> is given, it continues training from the <opt2:EPOCH>.
    - If <opt1:EXP_NAME> is given and pretrained model doesn't exist, fresh start training.
    - If <opt1:EXP_NAME> is not given, EXP_NAME will be generated by dd/hh/mm/ss.
    - If <opt1>, <opt2> and <opt3:TEST_MODE> are given:
        - If <opt3:TEST_MODE> == "test-only", it loads pre-trained model and proceed in-memory-search-test, then quit.
        - If <opt3:TEST_MODE> == "test", it tests while training.
        - If <opt3:TEST_MODE> == "db", it saves embeddings as a numpy file.
        
LOG-DIRECTORIES:
    

@author: skchang@cochlear.ai
"""
#%%
import sys, glob, os
import numpy as np
import tensorflow as tf
from datetime import datetime
from EATS import generator_fp
from EATS.networks.kapre2keras.melspectrogram import Melspectrogram
from utils.config_gpu_memory_lim import allow_gpu_memory_growth
from model.nnfp_l2norm import FingerPrinter
from model.online_triplet_v2 import Online_Batch_Triplet_Loss
allow_gpu_memory_growth(
)  # GPU config: This is required if target GPU has smaller vmemory

# Hyper-parameters
LOSS_MODE = 'all-balanced'
FEAT = 'melspec'  # 'spec' or 'melspec'
FS = 8000
DUR = 2
HOP = 1
LR = 2e-5  #2e-5
EMB_SZ = 128  #256 not-working now..
TR_BATCH_SZ = 80  #40
TR_N_ANCHOR = 16  # 8
TS_BATCH_SZ = 160
TS_N_ANCHOR = 32
TRIPLET_MARGIN = 0.2  #0.5
MAX_EPOCH = 500
EPOCH = ''
TEST_MODE = ''

# Directories
DATA_ROOT_DIR = '../fingerprint_dataset/music/'
AUG_ROOT_DIR = '../fingerprint_dataset/aug/'
IR_ROOT_DIR = '../fingerprint_dataset/ir/'

music_fps = glob.glob(DATA_ROOT_DIR + '**/*.wav', recursive=True)
aug_fps = glob.glob(AUG_ROOT_DIR + '**/*.wav', recursive=True)
ir_fps = glob.glob(IR_ROOT_DIR + '**/*.wav', recursive=True)
#%%
#EXP_NAME = 'exp_all_240_margin02'
#EPOCH = ''
#TEST_MODE = 'test-only'


if len(sys.argv) > 1:
    EXP_NAME = sys.argv[1]
if len(sys.argv) > 2:
    EPOCH = sys.argv[2]
if len(sys.argv) == 1:
    EXP_NAME = datetime.now().strftime("%Y%m%d-%H%M")
if len(sys.argv) > 3:
    TEST_MODE = sys.argv[3]
#%%
CHECKPOINT_SAVE_DIR = './logs/checkpoint/{}/'.format(EXP_NAME)
CHECKPOINT_N_HOUR = 1  # None: disable
LOG_DIR = "logs/fit/" + EXP_NAME
IMG_DIR = 'logs/images/' + EXP_NAME
EMB_DIR = 'logs/emb/' + EXP_NAME
os.makedirs(IMG_DIR, exist_ok=True)
os.makedirs(EMB_DIR, exist_ok=True)

# Dataloader
train_ds = generator_fp.genUnbalSequence(
    fns_event_list=music_fps[:8500],
    bsz=TR_BATCH_SZ,
    n_anchor= TR_N_ANCHOR, #ex) bsz=40, n_anchor=8: 4 positive samples per anchor 
    duration=DUR,  # duration in seconds
    hop=HOP,
    fs=FS,
    shuffle=True,
    random_offset=True,
    bg_mix_parameter=[True, aug_fps, (10, 10)],
    ir_mix_parameter=[True, ir_fps])

val_ds = generator_fp.genUnbalSequence(
    music_fps[8500:],
    TS_BATCH_SZ,
    TS_N_ANCHOR,
    DUR,
    HOP,
    FS,
    shuffle=False,
    random_offset=True,
    bg_mix_parameter=[True, aug_fps, (10, 10)],
    ir_mix_parameter=[True, ir_fps])

test_ds = val_ds

# Define Metric, Summary Log
tr_loss = tf.keras.metrics.Mean(name='train_loss')
ts_loss = tf.keras.metrics.Mean(name='test_loss')
tr_fraction = tf.keras.metrics.Mean(name='train_fraction')
tr_summary_writer = tf.summary.create_file_writer(LOG_DIR + '/train')
ts_summary_writer = tf.summary.create_file_writer(LOG_DIR + '/test')

# Build Model: m_pre, m_fp
input_aud = tf.keras.Input(shape=(1, FS * DUR))
mel = Melspectrogram(
    n_dft=1024,
    n_hop=256,
    sr=FS,
    n_mels=256,
    fmin=300,
    fmax=4000,
    return_decibel_melgram=True)(input_aud)
m_pre = tf.keras.Model(inputs=[input_aud], outputs=[mel])
m_pre.trainable = False
m_fp = FingerPrinter(emb_sz=EMB_SZ, fc_unit_dim=[63, 1])

# Define Optimizer & Loss
opt = tf.keras.optimizers.Adam(learning_rate=LR)
Online_Triplet_Loss_tr = Online_Batch_Triplet_Loss(
    TR_BATCH_SZ,
    TR_N_ANCHOR,
    n_pos_per_anchor=int((TR_BATCH_SZ - TR_N_ANCHOR) / TR_N_ANCHOR),
    use_anc_as_pos=True)
loss_obj_tr = Online_Triplet_Loss_tr.batch_triplet_loss_v2

Online_Triplet_Loss_ts = Online_Batch_Triplet_Loss(
    TS_BATCH_SZ,
    TS_N_ANCHOR,
    n_pos_per_anchor=int((TS_BATCH_SZ - TS_N_ANCHOR) / TS_N_ANCHOR),
    use_anc_as_pos=True)
loss_obj_ts = Online_Triplet_Loss_ts.batch_triplet_loss_v2

# Chekcpoint manager
checkpoint = tf.train.Checkpoint(optimizer=opt, model=m_fp)
c_manager = tf.train.CheckpointManager(checkpoint, CHECKPOINT_SAVE_DIR, 3,
                                       CHECKPOINT_N_HOUR)

if EPOCH == '':
    if c_manager.latest_checkpoint:
        tf.print("-----------Restoring from {}-----------".format(
            c_manager.latest_checkpoint))
        checkpoint.restore(c_manager.latest_checkpoint)
    else:
        tf.print("-----------Initializing from scratch-----------")
else:    
    checkpoint_fname = CHECKPOINT_SAVE_DIR + 'ckpt-' + str(EPOCH)
    tf.print("-----------Restoring from {}-----------".format(checkpoint_fname))
    checkpoint.restore(checkpoint_fname)
    



# Trainer
@tf.function
def train_step(Xa, Xp):  # {Xa: input_anchor, Xp: input_positive}
    X = tf.concat((Xa, Xp), axis=0)
    feat = m_pre(X)  # (nA+nP, F, T, 1)
    with tf.GradientTape() as t:
        emb = m_fp(feat)  # (B, E)
        loss = loss_obj_tr(
            emb_anchor=emb[:TR_N_ANCHOR, :],
            emb_pos=emb[TR_N_ANCHOR:, :],
            mode=LOSS_MODE,
            margin=TRIPLET_MARGIN,
            use_anc_as_pos=True,
            squared=False)
    g = t.gradient(loss, m_fp.trainable_variables)
    opt.apply_gradients(zip(g, m_fp.trainable_variables))
    
    # Logging
    tr_loss(loss)
    with tr_summary_writer.as_default():
        tf.summary.scalar('loss', tr_loss.result(), step=opt.iterations)
    return


@tf.function
def val_step(Xa, Xp):
    X = tf.concat((Xa, Xp), axis=0)
    feat = m_pre(X)
    emb = m_fp(feat) 
    loss = loss_obj_ts(
        emb_anchor=emb[:TS_N_ANCHOR, :],
        emb_pos=emb[TS_N_ANCHOR:, :],
        mode=LOSS_MODE,
        margin=TRIPLET_MARGIN,
        use_anc_as_pos=True,
        squared=False)
    
    # Logging
    ts_loss(loss)
    with ts_summary_writer.as_default():
        tf.summary.scalar('loss', ts_loss.result(), step=opt.iterations)
    return

#%%
@tf.function
def test_step(Xa, Xp):
    X = tf.concat((Xa, Xp), axis=0)
    feat = m_pre(X)
    emb = m_fp(feat)
    #return emb
    return tf.split(emb, [TS_N_ANCHOR, TS_BATCH_SZ - TS_N_ANCHOR], axis=0)  # emb_Anchor, emb_Pos


def search_test():
    db = np.empty((0, EMB_SZ))
    query = np.empty((0, int((TS_BATCH_SZ - TS_N_ANCHOR) / TS_N_ANCHOR), EMB_SZ))

    for Xa, Xp in test_ds:
        emb_anc, emb_pos = test_step(Xa, Xp)
        emb_pos = tf.reshape(emb_pos, (TS_N_ANCHOR, -1, EMB_SZ))  # ()

        db = np.concatenate((db, emb_anc.numpy()), axis=0)
        query = np.concatenate((query, emb_pos.numpy()), axis=0)

    return


# Main loop
def main():
    if TEST_MODE == 'test-only':
        search_test() # In-memory-search-test...
    else:
        start_ep = opt.iterations.numpy() // len(train_ds)
        
        for ep in range(start_ep, MAX_EPOCH):
            tr_loss.reset_states()
            ts_loss.reset_states()
            tf.print('EPOCH: {}/{}'.format(ep, MAX_EPOCH))
            progbar = tf.keras.utils.Progbar(len(train_ds))
    
            # Train
            for Xa, Xp in train_ds:
                train_step(Xa, Xp)
                progbar.add(1, values=[("tr loss", tr_loss.result())])
            c_manager.save()  # Save checkpoint...
    
            # Validate
            for Xa, Xp in val_ds:
                val_step(Xa, Xp)
            tf.print('trloss:{:.4f}, tsloss:{:.4f}'.format(tr_loss.result(),
                                                           ts_loss.result()))
            
            # Test 
            if TEST_MODE == 'test':
                search_test() # In-memory-search-test...

    return


if __name__ == "__main__":
    main()

#%%
#import wavio, glob, scipy
#for i in range(8):
#    wavio.write('Xa_{}.wav'.format(i), Xa[i,0,:], 8000, sampwidth=2)
#
#for i in range(32):
#    wavio.write('Xp_{}.wav'.format(i), Xp[i,0,:], 8000, sampwidth=2)

# Copyright (c) Cochlear.ai, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
""".

Created on Tue Apr 14 15:14:32 2020
@author: skchang@cochlear.ai

USAGE:
    CUDA_VISIBLE_DEVICES=0 python train_batch_hard.py <opt1:(str)EXP_NAME> <opt2:(int)EPOCH> <opt3:(str)TEST_MODE>

OPTIONS:
    - If <opt1:EXP_NAME> is given and pretrained model exists:
        - If <opt2> is empty, it continues training from the latest checkpoint.
        - Else if <opt2> is given, it continues training from the <opt2:EPOCH>.
    - If <opt1:EXP_NAME> is given and pretrained model doesn't exist, fresh start training.
    - If <opt1:EXP_NAME> is not given, EXP_NAME will be generated by dd/hh/mm/ss.
    - If <opt1>, <opt2> and <opt3:TEST_MODE> are given:
        - If <opt3:TEST_MODE> == "mini-test-only", it loads pre-trained model and proceed in-memory-search-mini-test, then quit.
        - If <opt3:TEST_MODE> == "save-db", it loads pre-trained model and only saves embeddings for DB as a numpy file.
        - If <opt3:TEST_MODE> == "save-db-query", it loads pre-trained model and saves embeddings for DB and Query as numpy files.
LOG-DIRECTORIES:
    
CUDA_VISIBLE_DEVICES=0 python train_with_EATS_v2fix_batch_NTxent_simCLR_320win1_server.py exp_NTxent_simCLR_320_ln_drop_lamb_sch1e03
CUDA_VISIBLE_DEVICES=0 python train_with_EATS_v2fix_batch_NTxent_simCLR_320win1_server.py exp_NTxent_simCLR_use_anc_rep_320 '1' mini-test-only
CUDA_VISIBLE_DEVICES=1 python train_with_EATS_v2fix_batch_NTxent_simCLR_320win1_server.py exp_NTxent_simCLR_use_anc_rep_320 '19' save-db-query # 3:99.39
CUDA_VISIBLE_DEVICES=1 python eval.py logs/emb/exp_NTxent_simCLR_use_anc_rep_320/19 100

@author: skchang@cochlear.ai
"""
#%%
import sys, glob, os, time
import numpy as np
import tensorflow as tf
from datetime import datetime
from EATS import generator_fp
from EATS.networks.kapre2keras.melspectrogram import Melspectrogram
from utils.plotter import save_imshow, get_imshow_image
from utils.config_gpu_memory_lim import allow_gpu_memory_growth, config_gpu_memory_limit
from model.nnfp_l2norm_v2_fixed import FingerPrinter
from model.online_NTxent_variant_loss import OnlineNTxentVariantLoss
#from model.online_triplet_v2_fixed import Online_Batch_Triplet_Loss
from utils.eval_metric import eval_mini 
allow_gpu_memory_growth()  # GPU config: This is required if target GPU has smaller vmemory
#from model.lamb_optimizer import LAMB
from tensorflow_addons.optimizers.lamb import LAMB

# NTxent-parameters
TAU = 0.05 # 0.1 #1. #0.3
REP_ORG_WEIGHTS = 1.0
LOSS_MODE = 'simCLR_use_anc_rep_mod' #'simCLR_use_anc_rep' # 
SAVE_IMG = True
BN = 'layer_norm2d' #'batch_norm'
OPTIMIZER = LAMB
LR = 1e-4#1e-4#5e-5#3e-5  #2e-5
LABEL_SMOOTH = 0
LR_SCHEDULE = ''

# Hyper-parameters
FEAT = 'melspec'  # 'spec' or 'melspec'
FS = 8000
DUR = 1
HOP = .5
EMB_SZ = 256  #256 not-working now..
TR_BATCH_SZ = 640#320 #120#240 #320 #80  #40
TR_N_ANCHOR = 320#160 #60 #120 #64 #16  # 8
VAL_BATCH_SZ = 120
VAL_N_ANCHOR = 60
TS_BATCH_SZ = 160 #160
TS_N_ANCHOR = 32 #80
MAX_EPOCH = 100
EPOCH = ''
TEST_MODE = ''

# Directories
DATA_ROOT_DIR = '../fingerprint_dataset/music/'
AUG_ROOT_DIR = '../fingerprint_dataset/aug/'
IR_ROOT_DIR = '../fingerprint_dataset/ir/'

music_fps = sorted(glob.glob(DATA_ROOT_DIR + '**/*.wav', recursive=True))
aug_tr_fps = sorted(glob.glob(AUG_ROOT_DIR + 'tr/**/*.wav', recursive=True))
aug_ts_fps = sorted(glob.glob(AUG_ROOT_DIR + 'ts/**/*.wav', recursive=True))
ir_fps = sorted(glob.glob(IR_ROOT_DIR + '**/*.wav', recursive=True))
#%%
#EXP_NAME = 'exp_NTxent_simCLR_use_anc_rep_120_tau0p05'
#EPOCH = 39
TEST_MODE = 'mini-test'


if len(sys.argv) > 1:
    EXP_NAME = sys.argv[1]
if len(sys.argv) > 2:
    EPOCH = sys.argv[2]
if len(sys.argv) == 1:
    EXP_NAME = datetime.now().strftime("%Y%m%d-%H%M")
if len(sys.argv) > 3:
    TEST_MODE = sys.argv[3]
#%%
CHECKPOINT_SAVE_DIR = './logs/checkpoint/{}/'.format(EXP_NAME)
CHECKPOINT_N_HOUR = 1  # None: disable
LOG_DIR = "logs/fit/" + EXP_NAME
IMG_DIR = 'logs/images/' + EXP_NAME
EMB_DIR = 'logs/emb/' + EXP_NAME
os.makedirs(IMG_DIR, exist_ok=True)
os.makedirs(EMB_DIR, exist_ok=True)

# Dataloader
def get_train_ds():
    ds = generator_fp.genUnbalSequence(
        fns_event_list=music_fps[:8500],
        bsz=TR_BATCH_SZ,
        n_anchor= TR_N_ANCHOR, #ex) bsz=40, n_anchor=8: 4 positive samples per anchor 
        duration=DUR,  # duration in seconds
        hop=HOP,
        fs=FS,
        shuffle=True,
        random_offset=True,
        bg_mix_parameter=[True, aug_tr_fps, (10, 10)],
        ir_mix_parameter=[True, ir_fps])
    return ds

def get_val_ds():
    ds = generator_fp.genUnbalSequence(
        music_fps[8500:],
        VAL_BATCH_SZ,
        VAL_N_ANCHOR,
        DUR,
        HOP,
        FS,
        shuffle=False,
        random_offset=True,
        bg_mix_parameter=[True, aug_ts_fps, (10, 10)],
        ir_mix_parameter=[True, ir_fps])
    return ds

def get_test_ds():
    ds = generator_fp.genUnbalSequence(
        list(reversed(music_fps)), # reversed list for later on evaluation!!
        TS_BATCH_SZ,
        TS_N_ANCHOR,
        DUR,
        HOP,
        FS,
        shuffle=False,
        random_offset=False,
        bg_mix_parameter=[True, aug_ts_fps, (10, 10)],
        ir_mix_parameter=[True, ir_fps])
    return ds

# Define Metric, Summary Log
tr_loss = tf.keras.metrics.Mean(name='train_loss')
val_loss = tf.keras.metrics.Mean(name='val_loss')
tr_summary_writer = tf.summary.create_file_writer(LOG_DIR + '/train')
val_summary_writer = tf.summary.create_file_writer(LOG_DIR + '/val')
#ts_summary_writer = tf.summary.create_file_writer(LOG_DIR + '/mini_test')
ts_summary_writer_dict = dict()
for key in ['gf', 'f', 'f_postL2']: 
    ts_summary_writer_dict[key] = tf.summary.create_file_writer(LOG_DIR + '/mini_test/' + key)
image_writer = tf.summary.create_file_writer(LOG_DIR + '/images')

# Build Model: m_pre, m_fp
input_aud = tf.keras.Input(shape=(1, FS * DUR))
mel = Melspectrogram(
    n_dft=1024,
    n_hop=256,
    sr=FS,
    n_mels=256,
    fmin=300,
    fmax=4000,
    return_decibel_melgram=True)(input_aud)
m_pre = tf.keras.Model(inputs=[input_aud], outputs=[mel])
m_pre.trainable = False
m_fp = FingerPrinter(emb_sz=EMB_SZ, fc_unit_dim=[32, 1], norm=BN)

# Define Optimizer & Loss
if LR_SCHEDULE == 'cos':
    lr_schedule = tf.keras.experimental.LinearCosineDecay(initial_learning_rate=LR,
        decay_steps=len(get_train_ds()) * MAX_EPOCH, num_periods=0.5, alpha=0, beta=2e-08)
    opt = OPTIMIZER(learning_rate=lr_schedule)
elif LR_SCHEDULE == 'cos-restart':
    lr_schedule = tf.keras.experimental.CosineDecayRestarts(initial_learning_rate=LR,
        first_decay_steps=len(int(get_train_ds())*0.1), num_periods=0.5, alpha=2e-07)
    opt = OPTIMIZER(learning_rate=lr_schedule)
else:
    opt = OPTIMIZER(learning_rate=LR)
TR_N_REP = TR_BATCH_SZ - TR_N_ANCHOR
assert(TR_N_REP==TR_N_ANCHOR)
Online_NTxent_loss_tr = OnlineNTxentVariantLoss(
    n_org=TR_N_ANCHOR,
    n_rep=TR_N_REP,
    tau=TAU,
    rep_org_weights=REP_ORG_WEIGHTS,
    mode=LOSS_MODE,
    label_smooth=LABEL_SMOOTH)
loss_obj_tr = Online_NTxent_loss_tr.batch_NTxent_loss

VAL_N_REP = VAL_BATCH_SZ - VAL_N_ANCHOR
assert(VAL_N_REP==VAL_N_ANCHOR)
Online_NTxent_loss_val = OnlineNTxentVariantLoss(
    n_org=VAL_N_ANCHOR,
    n_rep=VAL_N_REP,
    tau=TAU,
    rep_org_weights=REP_ORG_WEIGHTS,
    mode=LOSS_MODE,
    label_smooth=LABEL_SMOOTH)
loss_obj_ts = Online_NTxent_loss_val.batch_NTxent_loss


# Chekcpoint manager
checkpoint = tf.train.Checkpoint(optimizer=opt, model=m_fp)
c_manager = tf.train.CheckpointManager(checkpoint, CHECKPOINT_SAVE_DIR, 3,
                                       CHECKPOINT_N_HOUR)

if EPOCH == '':
    if c_manager.latest_checkpoint:
        tf.print("-----------Restoring from {}-----------".format(
            c_manager.latest_checkpoint))
        checkpoint.restore(c_manager.latest_checkpoint)
        EPOCH = c_manager.latest_checkpoint.split(sep='ckpt-')[-1]
    else:
        tf.print("-----------Initializing from scratch-----------")
else:    
    checkpoint_fname = CHECKPOINT_SAVE_DIR + 'ckpt-' + str(EPOCH)
    tf.print("-----------Restoring from {}-----------".format(checkpoint_fname))
    checkpoint.restore(checkpoint_fname)
    



# Trainer
@tf.function
def train_step(Xa, Xp):
    """Train step. Argument: {Xa: input_anchor, Xp: input_positive}."""
    X = tf.concat((Xa, Xp), axis=0)
    feat = m_pre(X)  # (nA+nP, F, T, 1)
    m_fp.trainable = True
    with tf.GradientTape() as t:
        emb = m_fp(feat)  # (B, E)
        loss, sim_mtx, _ = loss_obj_tr(emb_org=emb[:TR_N_ANCHOR, :],
                                 emb_rep=emb[TR_N_ANCHOR:, :])
    g = t.gradient(loss, m_fp.trainable_variables)
    opt.apply_gradients(zip(g, m_fp.trainable_variables))
    
    # Logging
    tr_loss(loss)
    with tr_summary_writer.as_default():
        tf.summary.scalar('loss', tr_loss.result(), step=opt.iterations)
    return sim_mtx


@tf.function
def val_step(Xa, Xp):
    """Validation step."""
    X = tf.concat((Xa, Xp), axis=0)
    feat = m_pre(X)
    m_fp.trainable = False
    emb = m_fp(feat) 
    loss, sim_mtx, _ = loss_obj_ts(emb_org=emb[:VAL_N_ANCHOR, :],
                       emb_rep=emb[VAL_N_ANCHOR:, :])
    
    # Logging
    val_loss(loss)
    with val_summary_writer.as_default():
        tf.summary.scalar('loss', val_loss.result(), step=opt.iterations)
    return sim_mtx


@tf.function
def test_step(Xa, Xp, drop_div_enc=False):
    """Test step."""
    X = tf.concat((Xa, Xp), axis=0)
    feat = m_pre(X)
    
    m_fp.trainable = False
    if drop_div_enc:
        emb = m_fp.front_conv(feat)  # Use f(x) as embedding:20.04.16.
    else:
        emb = m_fp(feat) # Use g(f(x))
    return tf.split(emb, [TS_N_ANCHOR, TS_BATCH_SZ - TS_N_ANCHOR], axis=0)  # emb_Anchor, emb_Pos

#%% test functions...
def generate_emb(n_query=100000):
    """
    Arguemnts:
        n_query: Number of embeddings for queries. If 0, then save only DB.
    
    Output Numpy memmap-files:
        logs/emb/<exp_name>/db.mm: (float32) tensor of shape (nFingerPrints, dim)
        logs/emb/<exp_name>/query.mm: (float32) tensor of shape (nFingerprints, nAugments, dim)
        logs/emb/<exp_name>/db_shape.npy: (int) 
        logs/emb/<exp_name>/query_shape.npy: (int)
    
    """
    n_augs_per_anchor = int((TS_BATCH_SZ - TS_N_ANCHOR) / TS_N_ANCHOR)
    n_augs_per_batch = n_augs_per_anchor * TS_N_ANCHOR
    n_query = (n_query // n_augs_per_batch) * n_augs_per_batch 
    n_batch_required_for_queries = int(n_query / n_augs_per_batch)
    
    test_ds = get_test_ds()
    shape_db = (len(test_ds) * TS_N_ANCHOR, EMB_SZ)
    shape_query = (n_query // n_augs_per_anchor, n_augs_per_anchor, EMB_SZ)
    
    # Create memmap, and save shapes
    os.makedirs(EMB_DIR + '/{}'.format(EPOCH), exist_ok=True)
    db = np.memmap(EMB_DIR + '/{}'.format(EPOCH) + '/db.mm', dtype='float32', mode='w+', shape=shape_db)
    np.save(EMB_DIR + '/{}'.format(EPOCH) + '/db_shape.npy', shape_db)
    if (n_query > 0):
        query = np.memmap(EMB_DIR + '/{}'.format(EPOCH) + '/query.mm', dtype='float32', mode='w+', shape=shape_query)
        np.save(EMB_DIR + '/{}'.format(EPOCH) + '/query_shape.npy', shape_query)
    
    progbar = tf.keras.utils.Progbar(len(test_ds))
    
    # Collect embeddings for full-size on-disk DB search, using np.memmap 
    for i, (Xa, Xp) in enumerate(test_ds):
        progbar.update(i)
        emb_anc, emb_pos = test_step(Xa, Xp)
        db[i*TS_N_ANCHOR:(i+1)*TS_N_ANCHOR, :] = emb_anc.numpy()
        if (i < n_batch_required_for_queries) & (n_query != 0):
            emb_pos = tf.reshape(emb_pos,
                                 (TS_N_ANCHOR, n_augs_per_anchor, EMB_SZ))  # (B,4,128)
            query[i*TS_N_ANCHOR:(i+1)*TS_N_ANCHOR, :, : ] = emb_pos.numpy()
    
    tf.print('------Succesfully saved embeddings to {}-----'.format(EMB_DIR + '/{}'.format(EPOCH)))
    del(db) # close mmap-files
    if (n_query > 0): del(query)
    return


def mini_search_test(mode='argmin', sel_emb='f', post_L2=False,
                     scopes=[1, 3, 5, 9, 11, 19], save=False, n_samples=3000):
    """Mini search test.
    - mode:'argmin' or 'argmax'
    - sel_emb: 'gf' uses g(f(.)) as embedding. 'f' uses f(.) as embedding.
    """
    test_ds = get_test_ds()
    if sel_emb=='gf':
        drop_div_enc = False
    elif sel_emb=='f':
        drop_div_enc = True
    else:
        raise NotImplementedError(sel_emb)
    
    # Collect mini DB
    db, query = np.empty(0), np.empty(0)
    n_iter = n_samples // TS_BATCH_SZ
    for i in range(n_iter):
        Xa, Xp = test_ds.__getitem__(i)
        emb_anc, emb_pos = test_step(Xa, Xp, drop_div_enc)
        # Post L2 normalize
        if post_L2:
            emb_anc = tf.math.l2_normalize(emb_anc, axis=-1)
            emb_pos = tf.math.l2_normalize(emb_pos, axis=-1)
        emb_pos = tf.reshape(emb_pos, (TS_N_ANCHOR, -1, emb_pos.shape[-1])) # (nAnc, nExam, dEmb)
        db = np.concatenate((db, emb_anc.numpy()), axis=0) if db.size else emb_anc.numpy()
        query = np.concatenate((query, emb_pos.numpy()), axis=0) if query.size else emb_pos.numpy()
    
    # Search test
    accs_by_scope, avg_rank_by_scope = eval_mini(query, db, mode=mode, display=True)
    # Write search test result
    if save:
        key = sel_emb + ('_postL2' if post_L2==True else '')
        with ts_summary_writer_dict[key].as_default():
            for acc, scope in list(zip(accs_by_scope[0], scopes)): # [0] is top1_acc
                tf.summary.scalar(f'acc_{scope}s', acc, step=opt.iterations)
    return


# Main loop
def main():
    if TEST_MODE == 'mini-test-only':
        tf.print('---------ArgMin g(f(x)) mini-TEST----------')
        mini_search_test(sel_emb='gf') # In-memory-search-test...
        tf.print('-----------ArgMin f(x) mini-TEST-----------')
        mini_search_test(sel_emb='f', post_L2=False)
        tf.print('---------ArgMin l2(f(x)) mini-TEST---------')     
        mini_search_test(sel_emb='f', post_L2=True)
    elif TEST_MODE == 'save-db':
        generate_emb(n_query=0)
    elif TEST_MODE == 'save-db-query':
        generate_emb(n_query=100000)
    else:
        n_iters = len(get_train_ds())
        start_ep = int(opt.iterations) // n_iters
        
        for ep in range(start_ep, MAX_EPOCH):
            tr_loss.reset_states()
            val_loss.reset_states()
            tf.print('EPOCH: {}/{}'.format(ep, MAX_EPOCH))
            progbar = tf.keras.utils.Progbar(n_iters)
    
            # Train
            """Parallelism to speed up preprocessing (2020-04-15)."""
            train_ds = get_train_ds()
            enq = tf.keras.utils.OrderedEnqueuer(train_ds, use_multiprocessing=True, shuffle=train_ds.shuffle)
            enq.start(workers=4, max_queue_size=10)

            i = opt.iterations.numpy() % n_iters # 0 unless using last checkpoint
            progbar.add(i)
            while i < len(enq.sequence):
                i += 1
                Xa, Xp = next(enq.get()) 
                sim_mtx = train_step(Xa, Xp)
                progbar.add(1, values=[("tr loss", tr_loss.result())])
                """Required for breaking enqueue."""
            enq.stop()
            """End of Parallelism................................."""
            if SAVE_IMG:
                img_sim_mtx = get_imshow_image(sim_mtx.numpy(), f'Epoch={ep}')
                img_softmax_mtx = get_imshow_image(tf.nn.softmax(sim_mtx,
                    axis=1).numpy(), title=f'Epoch={ep}')
                with image_writer.as_default():
                    tf.summary.image('tr_sim_mtx', img_sim_mtx, step=opt.iterations)
                    tf.summary.image('tr_softmax_mtx', img_softmax_mtx, step=opt.iterations)
            c_manager.save()  # Save checkpoint...
    
            # Validate
            for Xa, Xp in get_val_ds():
                sim_mtx = val_step(Xa, Xp)
            # Save image..
            if SAVE_IMG:
                img_sim_mtx = get_imshow_image(sim_mtx.numpy(), f'Epoch={ep}')
                img_softmax_mtx = get_imshow_image(tf.nn.softmax(sim_mtx,
                    axis=1).numpy(), title=f'Epoch={ep}')
                with image_writer.as_default():
                    tf.summary.image('val_sim_mtx', img_sim_mtx, step=opt.iterations)
                    tf.summary.image('val_softmax_mtx', img_softmax_mtx, step=opt.iterations)
                    
                
            tf.print('trloss:{:.4f}, tsloss:{:.4f}'.format(tr_loss.result(),
                                                            val_loss.result()))
            
            # Test 
            if TEST_MODE == 'mini-test':
                tf.print('---------ArgMin g(f(x)) mini-TEST----------')
                mini_search_test(sel_emb='gf', save=True) # In-memory-search-test...
                tf.print('-----------ArgMin f(x) mini-TEST-----------')
                mini_search_test(sel_emb='f', post_L2=False, save=True) # In-memory-search-test...
                tf.print('---------ArgMin l2(f(x)) mini-TEST---------')     
                mini_search_test(sel_emb='f', post_L2=True, save=True) # In-memory-search-test...
    return


if __name__ == "__main__":
    main()

##%%
#train_ds = generator_fp.genUnbalSequence(
#    fns_event_list=music_fps[:8500],
#    bsz=TR_BATCH_SZ,
#    n_anchor= TR_N_ANCHOR, #ex) bsz=40, n_anchor=8: 4 positive samples per anchor 
#    duration=DUR,  # duration in seconds
#    hop=HOP,
#    fs=FS,
#    shuffle=False,
#    random_offset=True,
#    bg_mix_parameter=[True, aug_fps, (10, 10)],
#    ir_mix_parameter=[True, ir_fps])    
#    
#    
#test_ds = generator_fp.genUnbalSequence(
#    music_fps,
#    TS_BATCH_SZ,
#    TS_N_ANCHOR,
#    DUR,
#    HOP,
#    FS,
#    shuffle=True,
#    random_offset=True,
#    bg_mix_parameter=[True, aug_fps, (10, 10)],
#    ir_mix_parameter=[True, ir_fps])    
#    
#    
#    
#import wavio, glob, scipy
#Xa, Xp = test_ds.__getitem__(20)
#for i in range(8):
#    wavio.write('Xa_bak{}.wav'.format(i), Xa[i,0,:], 8000, sampwidth=2)
#
#for i in range(32):
#    wavio.write('Xp_bak{}.wav'.format(i), Xp[i,0,:], 8000, sampwidth=2)
